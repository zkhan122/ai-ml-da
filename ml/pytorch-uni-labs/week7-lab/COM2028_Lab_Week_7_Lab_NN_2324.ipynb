{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9GCGMtwyohS"
      },
      "source": [
        "# Week 7: Convolutional Neural Networks (CNN)\n",
        "\n",
        "In this lab, we will cover the convolutional and pooling layers to create CNNs. <br>\n",
        "We also build a model to classify digit images using the MNIST dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRp_qWdxyohW"
      },
      "source": [
        "## Basic Imports\n",
        "\n",
        "For this lab session, we will use Numpy and PyTorch.<br>\n",
        "Please visit the [PyTorch Tutorial](https://pytorch.org/tutorials/) page if you want to learn it systematically.<br>\n",
        "Numpy will be mainly used for dataset preprocessing.\n",
        "\n",
        "### Use GPU for network training\n",
        "\n",
        "The use of a GPU card will accelerate the network training. To this end, you need to change your runtime type. You should click the \"Runtime\" menu at the top and then the \"Change runtime type\". Then you can select the “T4 GPU” and Save. <br>\n",
        "You should be able to print `cuda` using the code below if you have set your runtime correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5P4TN6uLyohX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# import numpy and torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VE6znDdhyohj"
      },
      "source": [
        "## Low Level Code\n",
        "\n",
        "While building a CNN model, we will add whole convolution layer in a neat package [Conv2d()](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html). <br>\n",
        "But before we do so, it is good to understand the __underlaying mechanics__ and __code implementation__ of convolutions. <br>\n",
        "In the following example, we will define our image/data `inputs`, and use `kernels` to apply 2D convolution. <br>\n",
        "\n",
        "\n",
        "For PyTorch, a convolution input must have the shape of `(BatchSize, inputChannels, height, width)` <br>\n",
        "A convolution filter/kernel must have shape of `(outputChannels, inputChannels, height, width)` <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMy6HP4Wyohi"
      },
      "source": [
        "# Convolutions\n",
        "\n",
        "Convolutional Neural Networks (CNNs) are designed to learn features directly from image pixels. They can classify patterns or objects with extreme variability. Currently, they form the core of various __computer vision systems__ such as Facebook's automated photo tagging, handwritten characters recognition, self-driving cars, marine mammal detection, and medical image analysis. In this lab, we will start by exploring a convolution function which forms the heart of CNNs.\n",
        "\n",
        "![image.png](https://global.discourse-cdn.com/business5/uploads/pynq1/original/2X/9/984a55715f7e36bdf068ee82d23a204d398f8561.jpeg)\n",
        "\n",
        "### Task\n",
        "1) Experiment with different strides, kernel, padding etc. <br>\n",
        "2) Why the shape of the output is 3 x 2? <br>\n",
        "3) Why does first ouput element equal to 21.0?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FJk4e3TyTQrX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of an Input: torch.Size([1, 1, 5, 5])\n",
            "Shape of a Kernel torch.Size([1, 1, 3, 3])\n",
            "Shape of result: torch.Size([1, 1, 3, 2])\n",
            "Result:\n",
            " [[[[21. 27.]\n",
            "   [36. 42.]\n",
            "   [51. 57.]]]]\n"
          ]
        }
      ],
      "source": [
        "# Use torch to create proper tensors\n",
        "inputs = torch.tensor([[1.0,  2.0, 3.0,  4.0, 5.0],\n",
        "             [6.0,  7.0, 8.0,  9.0, 10.0],\n",
        "             [11.0, 12.0, 13.0, 14.0, 15.0],\n",
        "             [16.0, 17.0, 18.0, 19.0, 20.0],\n",
        "             [21.0, 22.0, 23.0, 24.0, 25.0]])\n",
        "\n",
        "kernel = torch.tensor([[1.0, 0.0, 0.0],\n",
        "             [0.0, 1.0, 0.0],\n",
        "             [0.0, 0.0, 1.0]])\n",
        "\n",
        "# Reshape inputs and kernel to the required shape\n",
        "inputs = inputs.view(1, 1, 5, 5)\n",
        "print(\"Shape of an Input:\", inputs.shape)\n",
        "\n",
        "kernel = kernel.view(1, 1, 3, 3)\n",
        "print(\"Shape of a Kernel\", kernel.shape)\n",
        "\n",
        "stride = (1, 2)\n",
        "padding = (0, 0)\n",
        "# also try this\n",
        "# padding = 'valid'\n",
        "\n",
        "# Apply a 2D convolution over input to get result\n",
        "result = torch.nn.functional.conv2d(inputs, kernel, stride=stride, padding=padding)\n",
        "print(\"Shape of result:\", result.shape)\n",
        "\n",
        "print(\"Result:\\n\", result.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12ATZEXIyoho"
      },
      "source": [
        "# Pooling\n",
        "\n",
        "## MaxPooling\n",
        "\n",
        "Another important concept of CNNs is max-pooling, which is a form of\n",
        "non-linear down-sampling. Max-pooling partitions the input image into a\n",
        "set of non-overlapping rectangles and, for each such sub-region, outputs\n",
        "the maximum value.\n",
        "Max-pooling is useful in vision for two reasons:\n",
        "- By eliminating non-maximal values, it reduces computation for upper layers.\n",
        "- It provides a form of translation invariance.\n",
        "\n",
        "\n",
        "## AveragePooling\n",
        "Alternative to MaxPooling is Average pooling, where you take sum of all elements in pool and divide by number of elements.\n",
        "\n",
        "Experiment with different strides, kernel, padding etc. <br>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "520X_-ziyoho"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input shape: torch.Size([1, 1, 5, 5])\n",
            "Output shape: torch.Size([1, 1, 4, 4])\n",
            "Result:\n",
            " [[[[ 7.  8.  9. 10.]\n",
            "   [12. 13. 14. 15.]\n",
            "   [17. 18. 19. 20.]\n",
            "   [22. 23. 24. 25.]]]]\n"
          ]
        }
      ],
      "source": [
        "inputs = torch.tensor([[1.0,  2.0, 3.0,  4.0, 5.0],\n",
        "             [6.0,  7.0, 8.0,  9.0, 10.0],\n",
        "             [11.0, 12.0, 13.0, 14.0, 15.0],\n",
        "             [16.0, 17.0, 18.0, 19.0, 20.0],\n",
        "             [21.0, 22.0, 23.0, 24.0, 25.0]])\n",
        "\n",
        "inputs = inputs.view(1, 1, 5, 5)\n",
        "print(\"Input shape:\", inputs.shape)\n",
        "\n",
        "pool_size = (2, 2)\n",
        "stride = (1, 1)\n",
        "padding = (0, 0)\n",
        "\n",
        "#Apply Maxpooling\n",
        "#Try other pooling e.g. avg_pool2d https://pytorch.org/docs/stable/nn.functional.html#pooling-functions\n",
        "result = torch.nn.functional.max_pool2d(inputs, kernel_size=pool_size, stride=stride, padding=padding)\n",
        "\n",
        "print(\"Output shape:\", result.shape)\n",
        "print(\"Result:\\n\", result.numpy())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PZ0c4OppRLv"
      },
      "source": [
        "# Stride and Padding\n",
        "- You might have noticed the padding and stride, but do you know what the stride and padding exactly are?\n",
        "- Check this page for some visualisations https://ezyang.github.io/convolution-visualizer/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mohrx8gns73e"
      },
      "source": [
        "## <font color='red'>Task 1</font>\n",
        "We have a input of (1x1x4x4)\n",
        "- Can you make a Conv2D that give the output that is the same size as the input?\n",
        "- Can you make the output a half smaller than the original output? `(1x1x2x2)`\n",
        "- You can verify by printing out the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Cf7qUPM0s3zk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of an Input: torch.Size([1, 1, 4, 4])\n",
            "Shape of result:  torch.Size([1, 1, 4, 4])\n",
            "Shape of result:  torch.Size([1, 1, 2, 2])\n"
          ]
        }
      ],
      "source": [
        "inputs = torch.tensor([[1.0,  2.0, 3.0,  4.0],\n",
        "             [5.0,  6.0, 7.0,  8.0],\n",
        "             [9.0, 10.0, 11.0, 12.0],\n",
        "             [13.0, 14.0, 15.0, 16.0]])\n",
        "\n",
        "inputs = inputs.view(1, 1, 4, 4)\n",
        "print(\"Shape of an Input:\", inputs.shape)\n",
        "\n",
        "# Your code here (make the same size)\n",
        "# Make sure your output's shape is the same as the input 1x1x4x4\n",
        "\n",
        "kernel = torch.tensor([[1.0, 0.0, 0.0],\n",
        "                       [0.0, 1.0, 0.0],\n",
        "                       [0.0, 0.0, 1.0]])\n",
        "\n",
        "kernel = kernel.view(1, 1, 3, 3)\n",
        "\n",
        "stride = (1, 1)\n",
        "padding = (1, 1)\n",
        "\n",
        "result = torch.nn.functional.conv2d(inputs, kernel, stride=stride, padding=padding)\n",
        "print(\"Shape of result: \", result.shape)\n",
        "\n",
        "\n",
        "# Your code here (make a half smaller)\n",
        "# Make sure your output's shape is half of the input 1x1x2x2\n",
        "\n",
        "stride = (2, 2)\n",
        "padding = (1, 1)\n",
        "\n",
        "result = torch.nn.functional.conv2d(inputs, kernel, stride=stride, padding=padding)\n",
        "print(\"Shape of result: \", result.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4MxgHrMxvhd"
      },
      "source": [
        "# Using these techniques in a Pytorch model\n",
        "\n",
        "Pytorch provides convolution and [pooling](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html) layers, which can be used as-is, such as [Conv2D](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html).\n",
        "\n",
        "Pay close attention to the input and output shapes, as well as the optional parameters which may come in handy.\n",
        "\n",
        "We will walk through building a CNN classifier using the MNIST dataset.\n",
        "\n",
        "Load the dataset using Pytorch (hint [check the API](https://pytorch.org/vision/main/generated/torchvision.datasets.MNIST.html))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-DoX81DyzZd"
      },
      "source": [
        "### Data normalisation\n",
        "Note that neural networks perform __best when the input data ranges from 0 to 1__, or in some cases -1 to 1. <br>\n",
        "Therefore we need to divide our training and testing dataset by 255 to get values ranging from 0 to 1.\n",
        "\n",
        "In Pytorch, the ToTensor function converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255 ] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "sk-AN_dwyoht"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset MNIST\n",
            "    Number of datapoints: 60000\n",
            "    Root location: ./data\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               ToTensor()\n",
            "           )\n",
            "Dataset MNIST\n",
            "    Number of datapoints: 10000\n",
            "    Root location: ./data\n",
            "    Split: Test\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               ToTensor()\n",
            "           )\n"
          ]
        }
      ],
      "source": [
        "# Define the transform that converts the data to tensors https://pytorch.org/docs/stable/tensors.html\n",
        "# ToTensor converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255 ] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_data = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "print(train_data)\n",
        "print(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VYVCzZ7yohv"
      },
      "source": [
        "## Visualizing the dataset\n",
        "We will use __matplot library__ to display an image from the MNIST dataset. <br>\n",
        "\n",
        "Since we have __grayscale__ image, we need to specify that, while displaying it via `cmap='gray'`\n",
        "## <font color='red'>Task 1</font>\n",
        "- Please try to visualize more image data, so you can grasp what is in it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5zStjuHIyohw"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The above image is with the label of tensor(5)\n"
          ]
        }
      ],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Extract image and label from the training dataset\n",
        "image = train_data.data[0]\n",
        "label = train_data.targets[0]\n",
        "\n",
        "# Visualise the image and print the label\n",
        "# Note that the image has been rescaled to [0, 1] via ToTensor() in the Transform\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.show()\n",
        "print(\"The above image is with the label of \" + str(label))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QFMVC3S-oyI7"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The above image is with the label of tensor(0)\n"
          ]
        }
      ],
      "source": [
        "# Your code here, try to plot another image, maybe train_data[1] or other image you like\n",
        "image1 = train_data.data[1]\n",
        "label = train_data.targets[1]\n",
        "\n",
        "plt.imshow(image1, cmap=\"grey\")\n",
        "plt.show()\n",
        "print(\"The above image is with the label of \" + str(label))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Djhb9544ra4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0)\n",
            "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
          ]
        }
      ],
      "source": [
        "# Try to print the raw value from data, to see what is it like\n",
        "print(label)\n",
        "print(nn.functional.one_hot(label, num_classes=10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xkIR7OByohz"
      },
      "source": [
        "## Labels\n",
        "\n",
        "The label for each image is in form of an __integer__ ranging from 0 to 9. <br>\n",
        "We can use a __one hot encoding__ to transform the label into a __binary vector__. We know there are 10\n",
        "classes for this problem, so we can expect the binary vector to have 10 elements.\n",
        "\n",
        "### Converting labels to one-hot representation\n",
        "label <b>before</b> conversion is <b>[5]</b> <br>\n",
        "label <b>after</b> conversion is <b>[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5YpOp9q-yoh1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0)\n",
            "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
          ]
        }
      ],
      "source": [
        "#print the raw value of label\n",
        "print(label)\n",
        "\n",
        "#convert it to a one-hot vector\n",
        "label_one_hot = nn.functional.one_hot(label, num_classes=10)\n",
        "print(label_one_hot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4F-srsYtRzy"
      },
      "source": [
        "In PyTorch, the crossentropy function can deal with both labels.\n",
        "https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
        "<br>\n",
        "However, please note that you still need to output a vector with C elements for your network when dealing with a multi-class classfication problem. C is the number of classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "EOAYBpvUtH0k"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Actual class loss: tensor(2.4368)\n",
            "One hot vector encoding loss: tensor(2.4368)\n"
          ]
        }
      ],
      "source": [
        "# define the cross entropy loss function\n",
        "crossentropy = nn.CrossEntropyLoss()\n",
        "\n",
        "# given a prediction result of a network\n",
        "prediction = torch.Tensor([0, 0, 0, 0, 0.1, 0.8, 0.1, 0, 0, 0]) # ste the predictions (we\n",
        "                                                                # set highest to one we want -> 5 = 80%)\n",
        "\n",
        "# When the label is the actual class index (5)\n",
        "loss = crossentropy(prediction, label)\n",
        "print(\"Actual class loss:\", loss)\n",
        "\n",
        "# When the label is the one-hot vector\n",
        "loss = crossentropy(prediction, label_one_hot.type(torch.FloatTensor))\n",
        "print(\"One hot vector encoding loss:\", loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijrxqSVzyoh3"
      },
      "source": [
        "# Building the model\n",
        "In this section we will combine previously demonstrated operations into one network. <br>\n",
        "\n",
        "For this very simple model, we will be using [Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html), [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU) [Flatten](https://pytorch.org/docs/stable/generated/torch.flatten.html), [MaxPool2D](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html) and  [Fully connected (linear)](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear) layers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "cIuFd9mTyoh4"
      },
      "outputs": [],
      "source": [
        "class create_model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(create_model, self).__init__()\n",
        "        # Define the first convolutional layer with 1 input channel, 32 output channels, kernel size 3, and padding 1\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
        "        # ReLU activation\n",
        "        self.relu = nn.ReLU()\n",
        "        # Define the second convolutional layer with 32 input channels, 64 output channels, kernel size 3, and padding 1\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "        # Define the max pooling layer with kernel size 2 and stride 2\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        # Flatten layer\n",
        "        self.flatten = nn.Flatten()\n",
        "        # Define the first fully connected layer with 64*7*7 input features and 128 output features\n",
        "        self.fc1 = nn.Linear(64*7*7, 128)\n",
        "        # Define the second fully connected layer with 128 input features and 10 output features(number of classes)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass through first convolutional layer and activation function\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x)\n",
        "        # Pass through second convolutional layer and activation function\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x)\n",
        "        # Flatten the output\n",
        "        x = self.flatten(x)\n",
        "        # Add two FC layers\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "wf0bR7ZCyoh5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch-0 on cuda: 100%|██████████| 750/750 [00:09<00:00, 80.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train - Loss: 1.414646, Accuracy: 61.97%\n",
            "Validation - Loss: 0.431635, Accuracy: 86.67%\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch-1 on cuda: 100%|██████████| 750/750 [00:08<00:00, 84.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train - Loss: 0.340734, Accuracy: 89.50%\n",
            "Validation - Loss: 0.281808, Accuracy: 91.26%\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch-2 on cuda: 100%|██████████| 750/750 [00:09<00:00, 78.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train - Loss: 0.245934, Accuracy: 92.38%\n",
            "Validation - Loss: 0.214175, Accuracy: 93.38%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Model/data parameters\n",
        "epochs = 3\n",
        "batch_size = 64\n",
        "num_classes = 10\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Split the training dataset into train and validation sets\n",
        "train_size = int(0.8 * len(train_data))\n",
        "val_size = len(train_data) - train_size\n",
        "train_set, valid_set = torch.utils.data.random_split(train_data, [train_size, val_size])\n",
        "\n",
        "# Data_loader are created to provides an efficient and flexible way to load data into a model\n",
        "# for training or inference.\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=batch_size, shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "#Send model to the GPU device\n",
        "model = create_model().to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "crossentropy = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "# Calculate the accuracy given a set of predicted outputs (logits) and the label\n",
        "def accuracy(output, target):\n",
        "    pred = output.argmax(dim=1, keepdim=True)\n",
        "    correct = pred.eq(target.view_as(pred)).sum().item()\n",
        "    return correct / len(target)\n",
        "\n",
        "\n",
        "# For loop over epoches\n",
        "for epoch in range(epochs):\n",
        "    # Switch to train mode\n",
        "    model.train()\n",
        "\n",
        "    # running loss and accuracy\n",
        "    running_loss = 0.0\n",
        "    running_accuracy = 0.0\n",
        "\n",
        "    # For loop over mini batches\n",
        "    for inputs, targets in tqdm(train_loader, desc = \"Epoch-\" + str(epoch) + \" on \" + str(device)):\n",
        "        # Send data to GPU\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # In pytorch, always set the gradients to zero for each mini batch\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward\n",
        "        output = model(inputs)\n",
        "\n",
        "        # Loss\n",
        "        loss = crossentropy(output, targets)\n",
        "\n",
        "        # Backward\n",
        "        loss.backward()\n",
        "\n",
        "        # Model parameters update\n",
        "        optimizer.step()\n",
        "\n",
        "        # Upate running loss and accuracy\n",
        "        running_loss += loss.item()\n",
        "        running_accuracy += accuracy(output, targets)\n",
        "\n",
        "    print('Train - Loss: {:.6f}, Accuracy: {:.2f}%'.format(\n",
        "            running_loss / len(train_loader), running_accuracy / len(train_loader) * 100))\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    valid_loss = 0\n",
        "    valid_accuracy = 0\n",
        "    with torch.no_grad():\n",
        "        for data, label in valid_loader:\n",
        "            # Send data to GPU\n",
        "            data, label = data.to(device), label.to(device)\n",
        "            output = model(data)\n",
        "            valid_loss += crossentropy(output, label).item()\n",
        "            valid_accuracy += accuracy(output, label)\n",
        "\n",
        "    print('Validation - Loss: {:.6f}, Accuracy: {:.2f}%\\n'.format(\n",
        "        valid_loss / len(valid_loader), valid_accuracy / len(valid_loader) * 100))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTJteZVqTQra"
      },
      "source": [
        "The neural network is trained. You can use it to make predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Q6Ug5ymoTQra"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total accuracy on the test set: 93.82961783439491\n"
          ]
        }
      ],
      "source": [
        "# Switch model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Perform inference on the test set\n",
        "test_acc = 0.0\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        output = model(inputs)\n",
        "        test_acc += accuracy(output, targets)\n",
        "\n",
        "print(\"Total accuracy on the test set:\", test_acc/ len(test_loader) * 100 )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXn5Tj5LQ_Ci"
      },
      "source": [
        "## <font color='red'>Task 2</font>\n",
        "\n",
        "Now, try to create a new model by yourself and train the model. <br>\n",
        "Try to get a higher testing accuracy, e.g., 95%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset MNIST\n",
            "    Number of datapoints: 60000\n",
            "    Root location: ./data\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               ToTensor()\n",
            "           )\n",
            "Dataset MNIST\n",
            "    Number of datapoints: 10000\n",
            "    Root location: ./data\n",
            "    Split: Test\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               ToTensor()\n",
            "           )\n"
          ]
        }
      ],
      "source": [
        "# ToTensor converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255 ] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_data = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "print(train_data)\n",
        "print(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "A8E-pZMXRKuk"
      },
      "outputs": [],
      "source": [
        "#Create your model\n",
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Network, self).__init__()\n",
        "\n",
        "        self.conv_layer1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
        "        # ReLU activation performed\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "        self.conv_layer2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "\n",
        "        # add another relu for increased non linearity\n",
        "        self.relu3 = nn.ReLU()\n",
        "        # self.conv_layer3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
        "        \n",
        "        # Pooling layer defined with kernel size 2 and stride of 2 to reduce size and get important parts\n",
        "        self.pool_layer = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        # Flatten the layer\n",
        "        self.flatten = nn.Flatten()\n",
        "        # define the first fully connected layer with 64*7*7 and 128 output features\n",
        "        self.fc1 = nn.Linear(64*7*7, 128) # reduce size from 28*28 spatial resolution to 7*7\n",
        "        # Define the second fully connected layer with 128 input features and 10 output features(number of classes)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # first pass\n",
        "        x = self.conv_layer1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool_layer(x)\n",
        "        # second pass\n",
        "        x = self.conv_layer2(x)\n",
        "        x = self.relu2(x)\n",
        "        # third pass\n",
        "        # x = self.conv_layer3(x)\n",
        "        x = self.pool_layer(x)\n",
        "        # Flatten the output\n",
        "        x = self.flatten(x)\n",
        "        # Add the 2 FC layers\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "#Train the model on MNIST\n",
        "\n",
        "\n",
        "#Test the accuracy of the trained model on the test set\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch-0 on cuda:   0%|          | 0/480 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch-0 on cuda: 100%|██████████| 480/480 [00:08<00:00, 54.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train - Loss: 0.986200, Accuracy: 73.68%\n",
            "Validation - Loss: 0.324197, Accuracy: 90.37%\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch-1 on cuda: 100%|██████████| 480/480 [00:08<00:00, 57.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train - Loss: 0.277772, Accuracy: 91.59%\n",
            "Validation - Loss: 0.201698, Accuracy: 94.01%\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch-2 on cuda: 100%|██████████| 480/480 [00:10<00:00, 47.87it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train - Loss: 0.186230, Accuracy: 94.40%\n",
            "Validation - Loss: 0.146091, Accuracy: 95.78%\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch-3 on cuda: 100%|██████████| 480/480 [00:09<00:00, 51.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train - Loss: 0.139159, Accuracy: 95.87%\n",
            "Validation - Loss: 0.117388, Accuracy: 96.46%\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch-4 on cuda: 100%|██████████| 480/480 [00:08<00:00, 56.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train - Loss: 0.110786, Accuracy: 96.74%\n",
            "Validation - Loss: 0.096874, Accuracy: 96.96%\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch-5 on cuda: 100%|██████████| 480/480 [00:08<00:00, 57.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train - Loss: 0.093991, Accuracy: 97.17%\n",
            "Validation - Loss: 0.078214, Accuracy: 97.50%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "epochs = 6\n",
        "batch_size = 100\n",
        "num_classes = 10\n",
        "learning_rate = 0.02\n",
        "\n",
        "# split the data into training and validation\n",
        "train_size = int(0.8 * len(train_data)) # 80% for training\n",
        "validation_size = len(train_data) - train_size # 20% for validation\n",
        "training_set, validation_set = torch.utils.data.random_split(train_data, [train_size, validation_size])\n",
        "\n",
        "# create DataLoader pipelines\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "valid_loader = torch.utils.data.DataLoader(validation_set, batch_size=batch_size, shuffle=False)\n",
        "valid_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Send the model to the GPU device to run on there\n",
        "model = Network().to(device=device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "crossentropyloss = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "def accuracy(output, target):\n",
        "    prediction = output.argmax(dim=1, keepdim=True)\n",
        "    correct = prediction.eq(target.view_as(prediction)).sum().item()\n",
        "    return correct / len(target)\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # switch to train model\n",
        "    model.train()\n",
        "\n",
        "    # running loss and accuracy\n",
        "    running_loss = 0.0\n",
        "    running_accuracy = 0.0\n",
        "\n",
        "        # For loop over mini batches\n",
        "    for inputs, targets in tqdm(train_loader, desc = \"Epoch-\" + str(epoch) + \" on \" + str(device)):\n",
        "        # send the data to GPU\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # set gradients to 0 for each mini batch\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward\n",
        "        output = model(inputs)\n",
        "\n",
        "        # Loss\n",
        "        loss = crossentropy(output, targets)\n",
        "\n",
        "        # Backward\n",
        "        loss.backward()\n",
        "\n",
        "        # Model parameters\n",
        "        optimizer.step() # perform single optimization step\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        running_accuracy += accuracy(output, targets)\n",
        "\n",
        "\n",
        "    print('Train - Loss: {:.6f}, Accuracy: {:.2f}%'.format(\n",
        "    running_loss / len(train_loader), running_accuracy / len(train_loader) * 100))\n",
        "\n",
        "\n",
        "        # Validation loop\n",
        "    model.eval()\n",
        "    valid_loss = 0\n",
        "    valid_accuracy = 0\n",
        "    with torch.no_grad():\n",
        "        for data, label in valid_loader:\n",
        "            # Send data to GPU\n",
        "            data, label = data.to(device), label.to(device)\n",
        "            output = model(data)\n",
        "            valid_loss += crossentropy(output, label).item()\n",
        "            valid_accuracy += accuracy(output, label)\n",
        "\n",
        "    print('Validation - Loss: {:.6f}, Accuracy: {:.2f}%\\n'.format(\n",
        "        valid_loss / len(valid_loader), valid_accuracy / len(valid_loader) * 100))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3bdN9IvBh7S"
      },
      "source": [
        "\n",
        "### End of the notebook\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'NVIDIA GeForce GTX 1650'"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.get_device_name(0)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "nteract": {
      "version": "0.21.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
